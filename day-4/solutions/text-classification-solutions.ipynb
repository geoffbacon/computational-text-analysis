{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "\n",
    "In this notebook, you'll practice (almost) everything you've learnt in the workshop. You're going to read in a bunch of documents, perform preprocessing and EDA, and then train and evaluate a text classifier. Hopefully, you'll feel confident enough to do this largely by yourself, but feel free to refer back to previous notebooks or ask questions.\n",
    "\n",
    "### Data\n",
    "\n",
    "I've downloaded the \"Blog Authorship\" corpus from from [here](http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm). This is a corpus of 19,320 bloggers gathered from blogger.com in August 2004. The corpus has a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person. Each blog has been tagged with the blogger's (self-identified) gender, age, industry and astrological star sign. At a later time, I'd encourage you to read [the paper](http://u.cs.biu.ac.il/~schlerj/schler_springsymp06.pdf) that describes the corpus.\n",
    "\n",
    "Each blog is in a separate xml file. The names of the file indicate the blogger id in the corpus, then their gender, age, industry and start sign. Within the xml file, there are two tags: date and post. We're going to ignore the date tag. All the data we want is in the post tag.\n",
    "\n",
    "### Task\n",
    "There are lots of things you could do with this, but we're going to try to build a classifier to predict an blogger's age bracket.\n",
    "\n",
    "### Time\n",
    "- Teaching: 10 minutes\n",
    "- Exercises: 50 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from string import punctuation\n",
    "from xml.etree import ElementTree as ET\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data\n",
    "\n",
    "The first thing we want to do is read in all the data we'll need. We need both the text of the blog posts and the age of the blogger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/blogs'\n",
    "fname_pattern = os.path.join(DATA_DIR, '*.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_properties_from_fname(fname):\n",
    "    fname = os.path.basename(fname)\n",
    "    return fname.split('.')\n",
    "\n",
    "def extract_age_from_fname(fname):\n",
    "    properties = extract_properties_from_fname(fname)\n",
    "    age = int(properties[2])\n",
    "    return age\n",
    "\n",
    "def extract_gender_from_fname(fname):\n",
    "    properties = extract_properties_from_fname(fname)\n",
    "    gender = properties[1]\n",
    "    return gender\n",
    "\n",
    "def extract_id_from_fname(fname):\n",
    "    properties = extract_properties_from_fname(fname)\n",
    "    num = int(properties[0])\n",
    "    return num\n",
    "\n",
    "def extract_industry_from_fname(fname):\n",
    "    properties = extract_properties_from_fname(fname)\n",
    "    industry = properties[3]\n",
    "    return industry\n",
    "\n",
    "def extract_starsign_from_fname(fname):\n",
    "    properties = extract_properties_from_fname(fname)\n",
    "    starsign = properties[4]\n",
    "    return starsign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_text(fname):\n",
    "    e = ET.parse(fname)\n",
    "    root = e.getroot()\n",
    "    posts = root.findall('post')\n",
    "    text = [post.text for post in posts]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(fname):\n",
    "    response = extract_age_from_fname(fname)\n",
    "    num = extract_id_from_fname(fname)\n",
    "    try:\n",
    "        text = extract_all_text(fname)\n",
    "    except ET.ParseError:\n",
    "        text = np.NaN\n",
    "    return num, response, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob(fname_pattern)\n",
    "data = {}\n",
    "for fname in fnames[:1000]:\n",
    "    num, response, text = extract_data(fname)\n",
    "    data[num] = [response, text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "df.columns = ['age', 'text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove blogs with parsing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['text'].notnull()]\n",
    "df = df[df['age'].notnull()]\n",
    "df = df[df['age']<40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return ''.join([ch for ch in text if ch not in punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "    whitespace_pattern = r'\\s+'\n",
    "    no_whitespace = re.sub(whitespace_pattern, ' ', text)\n",
    "    return no_whitespace.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    url_pattern = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "    URL_SIGN = ' URL '\n",
    "    return re.sub(url_pattern, URL_SIGN, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digits(text):\n",
    "    digit_pattern = '\\d+'\n",
    "    DIGIT_SIGN = ' DIGIT '\n",
    "    return re.sub(digit_pattern, DIGIT_SIGN, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    try:\n",
    "        return word_tokenize(text)\n",
    "    except:\n",
    "        return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokenized_text = tokenize(text)\n",
    "    no_stopwords = [token for token in tokenized_text if token not in stops]\n",
    "    return ' '.join(no_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem(text):\n",
    "    tokenized_text = tokenize(text)\n",
    "    stems = [stemmer.stem(token) for token in tokenized_text]\n",
    "    return ' '.join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = text.lower()\n",
    "    text = remove_url(text)\n",
    "    text = remove_digits(text)\n",
    "    #text = remove_stopwords(text)\n",
    "    #text = stem(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['text'].apply(clean)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "TBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvectorizer = CountVectorizer(max_features=5000, binary=True)\n",
    "X = countvectorizer.fit_transform(df['clean_text'])\n",
    "features = X.toarray()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = list(range(10, 41, 10))\n",
    "labels = ['Under ' + str(i) for i in bins][:-1]\n",
    "response = pd.cut(df['age'], bins=bins, right=True, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, response, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logistic_regression(X_train, y_train):\n",
    "    model = LogisticRegressionCV(Cs=5, penalty='l1', cv=3, solver='liblinear', refit=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def conmat(model, X_test, y_test):\n",
    "    \"\"\"Wrapper for sklearn's confusion matrix.\"\"\"\n",
    "    labels = model.classes_\n",
    "    y_pred = model.predict(X_test)\n",
    "    c = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(c, annot=True, fmt='d', \n",
    "                xticklabels=labels, \n",
    "                yticklabels=labels, \n",
    "                cmap=\"YlGnBu\", cbar=False)\n",
    "    plt.ylabel('Ground truth')\n",
    "    plt.xlabel('Prediction')\n",
    "    \n",
    "def test_model(model, X_train, y_train):\n",
    "    conmat(model, X_test, y_test)\n",
    "    print('Accuracy: ', model.score(X_test, y_test))\n",
    "    \n",
    "def interpret(vectorizer, model):\n",
    "    vocab = [(v,k) for k,v in vectorizer.vocabulary_.items()]\n",
    "    vocab = sorted(vocab, key=lambda x: x[0])\n",
    "    vocab = [word for num,word in vocab]\n",
    "    coef = list(zip(vocab, model.coef_))\n",
    "    important = pd.DataFrame(lr.coef_).T\n",
    "    important.columns = model.classes_\n",
    "    important['word'] = vocab\n",
    "    return important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = fit_logistic_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(lr, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important = interpret(countvectorizer, lr)\n",
    "important.sort_values(by='Under 10', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important.sort_values(by='Under 20', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important.sort_values(by='Under 30', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
